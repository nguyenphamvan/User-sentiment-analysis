# -*- coding: utf-8 -*-

import pandas as pd
from pyvi import ViTokenizer
import re
import string
import codecs
import json
from string import punctuation

from sklearn.pipeline import Pipeline
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV
from gensim.models import Word2Vec
from gensim.models import KeyedVectors
from sklearn.externals import joblib

# T·ª´ ƒëi·ªÉn t√≠ch c·ª±c, ti√™u c·ª±c, ph·ªß ƒë·ªãnh
path_nag = 'sentiment_dicts/nag.txt'
path_pos = 'sentiment_dicts/pos.txt'
path_not = 'sentiment_dicts/not.txt'

with codecs.open(path_nag, 'r', encoding='UTF-8') as f:
    nag = f.readlines()
nag_list = [n.replace('\n', '').replace(' ', '_') for n in nag]

with codecs.open(path_pos, 'r', encoding='UTF-8') as f:
    pos = f.readlines()
pos_list = [n.replace('\n', '').replace(' ', '_') for n in pos]
with codecs.open(path_not, 'r', encoding='UTF-8') as f:
    not_ = f.readlines()
not_list = [n.replace('\n', '').replace(' ', '_') for n in not_]

negative_emoticons = {':(', '‚òπ', '‚ùå', 'üëé', 'üëπ', 'üíÄ', 'üî•', 'ü§î', 'üòè', 'üòê', 'üòë', 'üòí', 'üòì', 'üòî', 'üòï', 'üòñ',
                      'üòû', 'üòü', 'üò†', 'üò°', 'üò¢', 'üò£', 'üò§', 'üò•', 'üòß', 'üò®', 'üò©', 'üò™', 'üò´', 'üò≠', 'üò∞', 'üò±',
                      'üò≥', 'üòµ', 'üò∂', 'üòæ', 'üôÅ', 'üôè', 'üö´', '>:[', ':-(', ':(', ':-c', ':c', ':-<', ':„Å£C', ':<',
                      ':-[', ':[', ':{', ':((', ':(((', 'ü§¨', 'üö´'}

positive_emoticons = {'=))', 'v', ';)','))' , '^^', '<3', '‚ò∫', '‚ô°', '‚ô•', '‚úå', '‚ú®', '‚ù£', '‚ù§', 'üåù', 'üå∑', 'üå∏',
                      'üå∫', 'üåº', 'üçì', 'üéà', 'üêÖ', 'üê∂', 'üêæ', 'üëâ', 'üëå', 'üëç', 'üëè', 'üëª', 'üíÉ', 'üíÑ', 'üíã',
                      'üíå', 'üíé', 'üíê', 'üíì', 'üíï', 'üíñ', 'üíó', 'üíô', 'üíö', 'üíõ', 'üíú', 'üíû', ':-)', ':)', ':D', ':o)',
                      ':]', ':3', ':c)', ':>', '=]', '8)', ':)))', 'üòÜ', 'üòÇ', 'üëª','üò∏','üß°','üßö','‚òÄ','ü§©','ü§£',
                      'üñ§','üíØ','ü§ó','üôÇ','üòù','üòÑ','Ô∏èüÜóÔ∏è','üíü','üòú','üòõ','üòá', 'üòÉ','ü§™','üí™','üéâ'}


def remove_punctuation(_string):
    """x√≥a c√°c punctuation ra kh·ªèi t·ª´ v√† c√¢u"""
    # n·∫øu l√† k√Ω t·ª± ƒë·∫∑c bi·ªát th√¨ x√≥a
    for token in _string.split(' '):
        if token in punctuation:
            _string = _string.replace(token, '')
    return _string


def normalizeString(string):
    """T√°ch d·∫•u ra kh·ªèi t·ª´"""
    s = string.lower()
    # T√°ch d·∫•u c√¢u n·∫øu k√≠ t·ª± li·ªÅn nhau
    marks = '[.!?,-${}()]'
    r = "([" + "\\".join(marks) + "])"
    s = re.sub(r, r" \1 ", s)
    # Thay th·∫ø nhi·ªÅu spaces b·∫±ng 1 space.
    s = re.sub(r"\s+", r" ", s).strip()
    return s


def emoj_to_string(_string):
    """Chuy√™nr c√°c bi·ªÉu t∆∞·ª£ng c·∫£m x√∫c v·ªÅ text"""
    for token in _string.split(' '):
        if token in positive_emoticons:
            _string = _string.replace(token, 'positive')
        if token in negative_emoticons:
            _string = _string.replace(token, 'nagative')
    return _string


def remove_duplicate_characters(string):
    """Remove c√°c k√Ω t·ª± k√©o d√†i: vd: ƒë·∫πppppppp"""
    string = re.sub(r'([A-Z])\1+', lambda m: m.group(1), string, flags=re.IGNORECASE)
    return string


def replace_abbreviations(text):
    """Chu·∫©n h√≥a 1 s·ªë sentiment words/English words"""
    text = remove_duplicate_characters(text)

    replace_list = {
        ' √¥ k√™i ': ' ok ', ' okie ': ' ok ', ' o k√™ ': ' ok ',
        ' okey ': ' ok ', ' √¥k√™ ': ' ok ', ' oki ': ' ok ', ' oke ': ' ok ', ' okay ': ' ok ', ' ok√™ ': ' ok ',
        ' tks ': u' c√°m ∆°n ', ' thks ': u' c√°m ∆°n ', ' thanks ': u' c√°m ∆°n ', ' ths ': u' c√°m ∆°n ',
        ' thank ': u' c√°m ∆°n ',
        ' not ': u' kh√¥ng ', u' kg ': u' kh√¥ng ', ' kh ': u' kh√¥ng ', ' kp ': u' kh√¥ng ph·∫£i ', u' k√¥ ': u' kh√¥ng ',
        '"ko ': u' kh√¥ng ', u' ko ': u' kh√¥ng ', u' k ': u' kh√¥ng ', ' khong ': u' kh√¥ng ', u' hok ': u' kh√¥ng ',
        ' he he ': ' positive ', ' hehe ': ' positive ', ' hihi ': ' positive ', ' haha ': ' positive ',
        ' hjhj ': ' positive ',
        ' lol ': ' nagative ', ' cc ': ' nagative ', ' cute ': u' d·ªÖ th∆∞∆°ng ', ' huhu ': ' nagative ', ' vs ': u' v·ªõi ',
        ' wa ': ' qu√° ', ' w√° ': u' qu√° ', ' j ': u' g√¨ ', '‚Äú': ' ',
        ' sz ': u' c·ª° ', ' size ': u' c·ª° ', u' ƒëx ': u' ƒë∆∞·ª£c ', ' dc ': u' ƒë∆∞·ª£c ', ' ƒëk ': u' ƒë∆∞·ª£c ',
        ' ƒëc ': u' ƒë∆∞·ª£c ', ' authentic ': u' chu·∫©n ch√≠nh h√£ng ', u' aut ': u' chu·∫©n ch√≠nh h√£ng ',
        u' auth ': u' chu·∫©n ch√≠nh h√£ng ', ' thick ': u' positive ', ' store ': u' c·ª≠a h√†ng ',
        ' shop ': u' c·ª≠a h√†ng ', 'sp': u' s·∫£n ph·∫©m ', 'gud': u' t·ªët ', ' god ': u' t·ªët ', ' wel done ': ' t·ªët ',
        ' good ': u' t·ªët ', ' g√∫t ': u' t·ªët ',
        ' s·∫•u ': u' x·∫•u ', ' gut ': u' t·ªët ', u' tot ': u' t·ªët ', u' nice ': u' t·ªët ', ' perfect ': ' r·∫•t t·ªët ',
        'bt': u' b√¨nh th∆∞·ªùng ',
        ' time ': u' th·ªùi gian ', 'q√°': u' qu√° ', u' ship ': u' giao h√†ng ', u' m ': u' m√¨nh ', u' mik ': u' m√¨nh ',
        ' √™ Ãâ': '·ªÉ', 'product': 's·∫£n ph·∫©m', ' quality ': ' ch·∫•t l∆∞·ª£ng ', ' chat ': ' ch·∫•t ', ' excelent ': ' ho√†n h·∫£o ',
        ' bad ': ' t·ªá ', ' fresh ': ' t∆∞∆°i ', ' sad ': ' t·ªá ',
        ' date ': u' h·∫°n s·ª≠ d·ª•ng ', ' hsd ': u' h·∫°n s·ª≠ d·ª•ng ', ' quickly ': u' nhanh ', ' quick ': u' nhanh ',
        ' fast ': u' nhanh ', ' delivery ': u' giao h√†ng ', u' s√≠p ': u' giao h√†ng ',
        ' beautiful ': u' ƒë·∫πp tuy·ªát v·ªùi ', u' tl ': u' tr·∫£ l·ªùi ', u' r ': u' r·ªìi ', u' shopE ': u' c·ª≠a h√†ng ',
        u' order ': u' ƒë·∫∑t h√†ng ',
        ' ch·∫•t lg ': u' ch·∫•t l∆∞·ª£ng ', u' sd ': u' s·ª≠ d·ª•ng ', u' dt ': u' ƒëi·ªán tho·∫°i ', u' nt ': u' nh·∫Øn tin ',
        u' s√†i ': u' x√†i ', u' bjo ': u' bao gi·ªù ',
        ' thik ': u' th√≠ch ', u' sop ': u' c·ª≠a h√†ng ', ' fb ': ' facebook ', ' face ': ' facebook ', ' very ': u' r·∫•t ',
        u' qu·∫£ ng ': u' qu·∫£ng ',
        ' dep ': u' ƒë·∫πp ', u' xau ': u' x·∫•u ', ' delicious ': u' ngon ', u' h√†g ': u' h√†ng ', u' q·ªßa ': u' qu·∫£ ',
        ' iu ': u' y√™u ', ' fake ': u' gi·∫£ m·∫°o ', ' trl ': ' tr·∫£ l·ªùi ', ' >< ': u' positive ',
        ' por ': u' t·ªá ', ' poor ': u' t·ªá ', ' ib ': u' nh·∫Øn tin ', ' rep ': u' tr·∫£ l·ªùi ', u' fback ': ' feedback ',
        ' fedback ': ' feedback '}

    for k, v in replace_list.items():
        text = text.replace(k, v)
    # x√≥a s·ªë  ra kh·ªèi chu·ªói
    text = re.sub(r"\d+", "", text)

    return text


def normalize_text(text):
    """Chu·∫©n h√≥a chu·ªói"""
    # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
    text = text.lower().replace('\n', ' ')

    text = emoj_to_string(text)
    text = normalizeString(text)
    text = emoj_to_string(text)

    text = remove_punctuation(text)
    text = replace_abbreviations(text)

    # t√°ch t·ª´
    text = ViTokenizer.tokenize(text)
    texts = text.split()
    len_text = len(texts)
    for i in range(len_text):
        # X·ª≠ l√Ω v·∫•n ƒë·ªÅ ph·ªß ƒë·ªãnh
        # "M√≥n n√†y ch·∫≥ng ngon ch√∫t n√†o , k qu√° t·ªá --> m√≥n n√†y notpositive ch√∫t n√†o notnegative
        if texts[i] in not_list:
            for j in range(i,len_text-1):
                if texts[j + 1] in pos_list:
                    texts[i] = 'notpositive'
                    texts[j + 1] = ''
                    break

                if texts[j + 1] in nag_list:
                    texts[i] = 'notnagative'
                    texts[j + 1] = ''
                    break

        else:  # Th√™m feature cho nh·ªØng t·ª´ positive (m√≥n n√†y ngon--> m√≥n n√†y ngon positive)
            if texts[i] in pos_list:
                texts.append('positive')
            elif texts[i] in nag_list:
                texts.append('nagative')

    text = ' '.join([text for text in texts if text != ''])

    return text


def create_stopwords():
    stopwords = []
    with open('stopwords.txt', 'r') as f:
        for line in f.readlines():
            word = line.strip()
            stopwords.append('_'.join(word.split(' ')))
    f.close()
    return stopwords


def load_data_format(filename1):
    with open(filename1, 'r') as file:
        contents = json.load(file)
        file.close()
    return contents


def transform_to_dataset(x_set, y_set):
    X, y = [], []
    for document, topic in zip(list(x_set), list(y_set)):
        document = normalize_text(document)
        X.append(document.strip())
        y.append(topic)
    return X, y


def prepare_data():
    train_data = pd.DataFrame(load_data_format('data/data.json'))

    X_train, X_test, y_train, y_test = train_test_split(train_data.comment, train_data.label, test_size=0.3,
                                                        random_state=42)
    X_train, y_train = transform_to_dataset(X_train, y_train)
    X_test, y_test = transform_to_dataset(X_test, y_test)

    return X_train, y_train, X_test, y_test

def train_model(X_train, y_train, stopwords):
    tfidfVectorizer = TfidfVectorizer(analyzer='word', stop_words=stopwords, max_features=30000, max_df=0.5, min_df=5,
                                      ngram_range=(1, 3), norm='l2', smooth_idf=True)

    classifier_1 = LinearSVC(fit_intercept=True, C=1.4)
    classifier_2 = MultinomialNB(alpha=1.0)

    steps = [('tfidfVectorizer', tfidfVectorizer),
             ('classifier', classifier_1)]

    clf = Pipeline(steps)
    clf.fit(X_train, y_train)
    clf = joblib.dump('model.pkl')
    return clf



def test(file_path, model):
    test_data = pd.DataFrame(load_data_format(file_path))
    X_test, y_test = transform_to_dataset(test_data.comment, test_data.label)
    y_pred = model.predict(X_test)
    report1 = metrics.classification_report(y_test, y_pred, labels=[1, 0])
    classifier = model['classifier']
    print('Name of classifier : ', type(classifier).__name__)
    print(report1)


# X_train, y_train, X_test, y_test = prepare_data()
# stopwords = create_stopwords()

clf = joblib.load('model.pkl')
test('data/test_data.json', clf)
#
# while True:
#   print('H√£y nh·∫≠p v√†o 1 comment : ')
#   X_test = input()
#   X_test = normalize_text(X_test.strip())
#   print([X_test])
#
#   y_predict = clf.predict([X_test])
#   print('Comment ƒë∆∞·ª£c ph√¢n lo·∫°i l√† : ')
#   if int(y_predict[0]) == 0:
#     print('T√≠ch c·ª±c')
#   else:
#     print('Ti√™u c·ª±c')
#   print()


# def load_model():
#     import joblib
#     model = joblib.load('model.pkl')
#     return model
#
# def classify_one_comment(model,comment):
#     comment = normalize_text(comment)
#     print(comment)
#     predict = model.predict([comment])
#     return predict

